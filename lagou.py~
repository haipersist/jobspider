#!/usr/bin/python
# -*- coding:utf-8 -*-


from jobspider import Job_Spider
import re
import time



class LG_Spider(Job_Spider):

    def __init__(self,website):
        super(LG_Spider,self).__init__(website)


    def parse(self,url):      
        try:
            soup=self.get_html_content(url,self.headers)
        except:
            time.sleep(2)
            soup=self.get_html_content(url,self.headers)
        result=[]
        for jobs in soup.find('ul','hot_pos reset').children:
            try:
                item={}
                item.setdefault('company','')
                item.setdefault('jobintro','')
                pos_l=jobs.find('div','hot_pos_l')
                pos_r=jobs.find('div','hot_pos_r')
                item['title']=pos_l.div.a['title']
                item['link']=pos_l.div.a['href']
                intros=[]
                for intro in pos_l.find_all('span'):
                    intros.append(unicode(intro.get_text()))
                item['jobintro']=','.join(intros)
                item['date']=pos_l.find_all('span')[-1].get_text()
                item['company']=pos_r.find('div','mb10').a['title']               
                result.append(item)
            except:
                continue
        return result



    def pages_parse(self,keyword):
        for page in xrange(1,30):
            self.url='http://www.lagou.com/jobs/list_python?kd=%s&spc=1&pl=&gj=&xl=本科&yx=&gx=全职&st=&labelWords=&lc=&workAddress=&city=北京&requestId=i&pn=%d'%(keyword,page)
            yield self.url


            

            







if __name__=="__main__":

    lg=LG_Spider('lagou')
    print lg.site
    print lg.headers
    for url in lg.pages_parse('python'):
        print url
        lg.store(lg.parse(url))

    print lg.site
    print lg.headers

